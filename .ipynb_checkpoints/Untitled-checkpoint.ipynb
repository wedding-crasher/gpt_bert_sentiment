{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18aaffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 23:14:35.367520: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-24 23:14:36.021065: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import transformers \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments,get_scheduler\n",
    "from torch import nn, optim \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8791f4a3-eb31-4885-9ab8-8270bf1b8237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16616651465069773793\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 23334223872\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8100994724332483430\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\"\n",
      "xla_global_id: 416903419\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 23361880064\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11864928314094314858\n",
      "physical_device_desc: \"device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:02:00.0, compute capability: 8.9\"\n",
      "xla_global_id: 2144165316\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 23:14:36.599238: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 23:14:36.599454: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 23:14:36.603665: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 23:14:36.603851: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 23:14:36.604018: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 23:14:36.604181: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 23:14:36.624077: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 23:14:36.624276: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 23:14:36.624495: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 23:14:36.624674: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 23:14:36.624849: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 23:14:36.625005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /device:GPU:0 with 22253 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-05-24 23:14:36.625361: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 23:14:36.625515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /device:GPU:1 with 22279 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:02:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0eab043",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"heegyu/gpt2-emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c39eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('heegyu/gpt2-emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2535ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class movRevDataSet(Dataset):\n",
    "    def __init__(self,root_dir,transform = None):\n",
    "        self.root_dir = root_dir \n",
    "        self.transform = transform\n",
    "        self.file_list = os.listdir(root_dir)\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_list[idx]\n",
    "        file_path = os.path.join(self.root_dir, file_name)\n",
    "        with open(file_path, 'r', encoding = 'utf-8') as file:\n",
    "            txt = file.read()\n",
    "#         label = int(file_name.split('_')[1].split('.')[0])\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5856664",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataRoot = './aclImdb/train/merged'\n",
    "testDataRoot = './aclImdb/test/merged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6b761cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "movRevTrainDS = movRevDataSet(root_dir=trainDataRoot)\n",
    "movRevTrainDS, movRevValDS = torch.utils.data.random_split(movRevTrainDS,[0.8,0.2])\n",
    "movRevTestDS = movRevDataSet(root_dir= testDataRoot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb4abb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "movRevTrainDL= DataLoader(movRevTrainDS, batch_size = 2, shuffle= True, num_workers =8 )\n",
    "movRevValDL = DataLoader(movRevValDS, batch_size =2, shuffle = True,num_workers =8 )\n",
    "movRevTestDL = DataLoader(movRevTestDS, batch_size = 2, num_workers =8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbe60aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "['Home Alone 3 is one of my least favourite movies. It\\'s the cream of the crop, or s*** if you tend to be more cynical, as it ranks up (or down) there with stuff like Battlefield Earth and Flinstones: Viva Rock Vegas. In fact, it could even be worse than those two, since those two at least intermittently made me laugh at their stupidity. This just made me cringe in pain constantly and clap when the credits started rolling. No other movie has made me cringe in pain. Now I will point out exactly why this movie is so incredibly atrocious.<br /><br />First off, the plot is ridiculous. It revolves around a chip in a remote control car (?!) that is misplaced and how these terrorists want it. Dumb stuff.<br /><br />The action that ensues is similar to that of the other two Home Alones, with boobytraps and all, but watching these boobytraps being executed is, rather than being funny, incredibly unpleasant to watch. I didn\\'t laugh (or even so much as smile) once, rather, I cringed constantly and hoped that the terrorists would nail the kid. The bird, rather than providing comic relief, was unfunny and annoying.<br /><br />The acting, as done by a bunch of no names, ranges from poor to atrocious. There is not a single good performance here. Alex D.Linz is absolutely unlikeable and unfunny as the kid, while the terrorists act (and judging by their movie credits, look) as they\\'ve been hastily picked off the street...and well, that\\'s it.<br /><br />I can see some people saying: \"Man, it\\'s for the kids. Don\\'t dis it, man.\" Well MAN, kids may like this, but they can get a hell of a lot better. See Monsters Inc. and Toy Story before even considering getting this out. Hell, even Scooby Doo and Garfield (which suck - see those reviews for more) are better than this! <br /><br />So in short, this is an irredeemably atrocious movie. This was clearly recycled for the money, as it almost completely rips off the first two; the only thing is, it completely insults the first two as well. No human, kid or otherwise, should find any reason to see Home Alone 3. Ever. It\\'s THAT bad.<br /><br />0/5 stars', \"I was lucky enough to catch this movie while volunteering at the Maryland Film Festival. I've always been a fan of classic horror films and especially the gimmicks of William Castle, so this was definitely a must-see for me.<br /><br />This is about the life and work of William Castle, who in my opinion was an underrated director. True, he made some cheap budget schlocky horror films, but he added something to these films: real live theater gimmicks that you don't see anymore. For example, he had nurses in case someone had a heart attack at his movies and put vibrators at the bottoms of chairs in THE TINGLER.<br /><br />This is truly a well-made documentary and brings this rather shadowed director into the light, and celebrated his contributions to horror cinema. It also paints Castle as a larger than life character, who was very well-liked and had a smile on his face.<br /><br />Unlike most film documentaries that mostly show testaments from film historians, SPINE TINGLER! shows interviews mostly from his family members and directors who were influenced by his work, such as John Waters, John Landis, and Joe Dante. A must see for classic horror and sci-fi fans.\"]\n",
      "Sample 2:\n",
      "[\"A very engaging documentary about Scottish artist Andy Goldsworthy, whose work consists mostly of ephemeral sculptures made from elements from nature. His work is made of rocks, leaves, grass, ice, etc., that gets blown away when the tide arrives at the beach or the wind blows at the field. Thus, most of Goldsworthy's works don't really last, except as photos or films of what they were. Now, one can argue that Goldsworthy's works are a reflection of mortality, or words to that effect, but isn't it easier to say that what he does is just beautiful art. And at a time when the stereotype about artists is that they are mostly bitter, pretentious, often mentally unstable people who live in decrepit urban settings, Goldsworthy seems to be the opposite: a stable, unpretentious, family oriented person who loves nature and lives in a small village in Scotland (of course, I'm sure those are the same reasons why he's shunned by some people on the art world who found his works fluffy or superficial).\", 'Although not one of Vonnegut\\'s better known works, it is a definite \"must-see\". Interestingly thought out, I especially like how the director filmed the couple in love.']\n",
      "Sample 3:\n",
      "[\"First of all, I firmly believe that Norwegian movies are continually getting better. From the tedious emotional films of the 70's and 80's, movies from this place actually started to contain a bit of humour. Imagine.. Actual comedies were made! Movies were actually starting to get entertaining and funny, as opposed to long, dark, depressing and boring.<br /><br />During the 90's and 00's several really great movies were made by a 'new generation' of filmmakers. Movie after movie were praised by critics and played loads of money. It became the norm!<br /><br />Then came United...<br /><br />*MINOR SPOILERS* It's just simply not funny. Not once. Not ever. But the thing is... We THINK its funny. Because we're used to norwegian movies to be funny. Especially with a cast like this with a few really funny comedians. But.. They neither say nor do anything funny! Where's the humor? Show me the humor! Is it the awkward clerk played by Harald Eia? Is it the overacting totally ridiculously unrealistic football coach? Is it the commentaries by Arne Scheie? The movie is just not funny!<br /><br />But thats not my main rant about United. That namely is the predictability. (And it is here I fear that norwegian comedies have come to a standstill since I have seen this in many other movies as well.) All the time you just know its going to end well. All characters are exactly as they are presented in the start of the movie, and everybody gets exactly what they deserve in the end. There's absolutely no room for surprises at all!<br /><br />All in all I can say that I sat with a bad feeling after seeing this movie. It was the one movie that made me realize that we probably need some new blood in norwegian movie making... again!<br /><br />Rating: 1/6\", 'Though not a horror film in the traditional sense, this creepy little film delivers the goods. It seems a vampire is loose in a small German town draining its victims of their blood. Police Inspector Karl Brettschneider, Melvyn Douglas in one of his early roles, is skeptical believing a crazed killer not a vampire is running amok. The only one who believes him is Ruth Bertin (Faye Wray) the inspector\\'s girlfriend and lab assistant to Dr. Otto von Niemann (Lionel Atwill) who though apparently an eminent scientist goes along with the vampire theory. The townspeople suspect the weirdo Herman Gleib, played with his usual frenzy by Dwight Frye who seems to be having a lot of fun with his role. The film contains quite a bit of humor which helps relieve some of the intensity involved with all the murders being committed. One funny part has Gussie Schnappmann (Maude Eburne), Ruth Bertin\\'s aunt, thinking weird Herman has turned not into a bat but into a dog. Maude Eburne and Dwight Frye make a good comedy team.<br /><br />This budget movie brings in elements from \"The Cabinet of Dr. Caligari\" with Dr. Niemann using the power of suggestion to make a somnambulist carry out his orders, from \"Frankenstein\" by using the human blood to help create life in the laboratory, and \"Dracula\" since the murders are believed by everyone except the inspector and his girl to be the work of a bloodsucker. Thses elements are mixed well by director Frank R. Strayer with a little comedy thrown in for good measure. The concoction works. The restored version I viewed used tinting to increase the spooky atmosphere. So try to see the this version if possible.']\n",
      "Sample 4:\n",
      "['Stories about the possibility of a post-apocalyptic future have been around for ages, since the very creation of science-fiction as a genre per se. The fact that today\\'s society is responsible for what may become of the future in the near tomorrow, and that our own abuses and refusals to see what is right before out eyes are at the very center of all of these stories, whether they are good or bad.<br /><br />Terry Gilliam of course is a natural for this kind of film. He gives the movie a decadent feel throughout, showing a society run ragged by its own excesses and bringing forth the a sense of imminent tragedy despite having moments of comedy. His world, the world in which TWELVE MONKEYS transpires, is a place where the mad run wild, where cities are collapsing in filth and neglect, where everything reeks of foreboding despite the luminosity of the opening sequence, where madness looms at every corner. This is a very dark movie, but his very best, most linear (despite the plot twists which hold up under examination), and one which gets better with repeated viewings.<br /><br />A tragic event in which a deadly virus was unleashed onto humanity in 1996 and thus led to the extermination of Life On The Planet As We Have Known It leads to scientists of the future to try and make amends to change humanity\\'s fate on the Earth by employing renegade citizens -- the scum of the Earth -- as guinea pigs to go back in time, among them one James Cole (underplayed to great effect by Bruce Willis). Cole could be any person. We don\\'t know anything about him, but in a way, that doesn\\'t matter since he is little more than one of many expendable volunteers and hints of his character sneak in later as he gets closer to fulfilling his mission. What we do know is that he is a man who dreams, and his dreams may have been reality: he may have already been at the scene of the Event of 1996.<br /><br />It\\'s this constant sense of deja vu that keeps popping up throughout the movie. When taken to a mental ward by mistake in 1990 he meets Jeffrey Goines (spastically played by Brad Pitt, Oscar-nominated here) who frantically spews forth talk about doom and destruction, and later Cole believes he has seen Goines in his recurring dream as a man pushing a boy aside while escaping... what? He doesn\\'t know. Later he meets a psychologist, Kathryn Railly (Madeleine Stowe), and one of her first reactions to him is that he\\'s insane, and that she\\'s seen him before. This becomes a running notion throughout her participation in this story from passive/resistant to active and even slightly crazy believer that Something Terrible is coming This Way, especially when she meets him six years later: she has seen Cole before. At the same time, Cole continues talking about a dream he keeps having in which she also plays a part as a blonde woman running down the aisle, screaming for help, after shots have rung out and a particular red-headed man in a ponytail (Jeffrey Goines?) has apparently escaped, not before pushing the little boy who is an innocent bystander. The questions arise: have these events happened? Are they going to happen? Who is really a part of this, or better yet -- is everyone, down to the smallest player, a part of a Greater Plot? Or is this all some trick in the fabric of time in which Time in itself is one huge conveyor belt showing repetitions of fragments of events that slide by over and over again? <br /><br />These questions are formulated in a masterful sequence which includes key scenes of Alfred Hitchcock\\'s masterpiece VERTIGO in which Madeleine Elster/Judy Barton mourns her own brief existence (\"You took no notice,\" she says, as Cole and Railly watch from their seats in the movie theatre they are hiding in). Snippets of dialog from VERTIGO form a foil to the dialog between Railly and Cole and later, when Cole awakens from having apparently dozed off in the theatre and goes looking for Railly, he comes face-to-face with her in disguise (looking almost exactly like Eva Maria Saint from NORTH BY NORTHWEST) as the swelling Bernard Herrmann score plays the emergence of Judy Barton, dressed as Madeleine Elster. It\\'s a fascinating sequence, more so because of the most improbable occurrence of the names of the actors in both films: Madeleine Stowe plays Kathryn Railly who dons a blond wig and grey trench-coat and calls herself \"Judy Simmons\" while helping an \"insane\" man named James Cole; James Stewart plays a detective who tries to help \"insane\" Madeleine Elster who will later re-appear not once, but twice, first as brunette Judy Barton, and later, as Madeleine. Action and re-enaction, play and re-play.', \"Trapped: buried alive brings us to a resort that has just opened, and is soon to close.<br /><br />We start with a guy in gear blowing up drifts, to avoid the possibility of avalanches. somehow, that doesn't make sense. anyways, he's about to blow away one particularly big one, when he notices the resort is open. despite his best efforts, higher authority tells him his day is over.<br /><br />soon, as everyone expects, an avalanche hits.<br /><br />Look, i'm not gonna reveal any more, all i can say is this was a B-movie designed for the family channel (which i just saw it on, and the fact it had no commercials proves it's a B-movie) anyways, it's a pretty decent film, but it's partially unreal.<br /><br />firsthand, when people are buried by ice and snow, they're buried. not just traced by powder. or, what about a CD for a screwdriver? it's not possible. and finally, what i can't stress enough, is that an explosion cannot stop a avalanche, guaranteed.<br /><br />furthermore, it's worth a rental or a TV viewing, but not owning. 7/10.<br /><br />The movie is rated PG, but maybe it should have received something a little more strong. a boy nearly loses his foot in an elevator, but his leg is cut around the ankle, a guy is toasted by electricity and diesel, and in the weight room, dead people are laying around.<br /><br />enjoy.\"]\n",
      "Sample 5:\n",
      "[\"Paul Verhoeven's De Vierde Man (The Fourth Man) is one of the most compelling thrillers I have ever seen. It really was a pleasant surprise. The story concerns bi-sexual writer Gerard (Jeroen Krabbe), as he is lured into a relationship with beautiful hairdresser Christine (Renée Soutendijk), but in the twisted mind of Gerard there could be more to the story. Verhoeven and cinematographer Jan De Bont create a beautiful and thick atmosphere full of surreal and sickening sexual imagery, this really pulls you into the story, you don't want to watch, but you can't turn yourself away. This is by far Verhoeven's best film (maybe second only to Robocop). True The Fourth Man isn't for everyone, some of the sex scenes are quite gratuitous (just ignore them, but trust me, if you watch for at least ten minutes you'll be hooked. This is one of those films that you need to know how it ends, a true whodunit it in the Hitchcock tradition, compelling, controversial and thrilling. I even like the spider metaphor.<br /><br /> 8/10\", \"The premise of this anime series is about bread, of all things to base a plot on! I truly laughed. The main character has a special bread making power that he was born with, and he goes off to bread baking school. I wish it were available on DVD, and it doesn't matter if it's subtitled or dubbed - it's that good. Even the theme song alone is funny. At one point in the theme song, there's an African-Japanese man with an afro on horseback, wielding a French baguette as if it were a samurai sword. These images will not make sense unless you see the anime. You'll laugh until your sides hurt. It is definitely the most unique anime I have seen thus far.\"]\n"
     ]
    }
   ],
   "source": [
    "for i, (data) in enumerate(movRevTrainDL):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(data)\n",
    "#         print(\"Label:\", label)\n",
    "        if i == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82ff955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ca1b265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d52dac5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                               | 0/300000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),lr=2e-5)\n",
    "num_epochs = 30\n",
    "num_training_steps = num_epochs * len(movRevTrainDL)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "lr_scheduler = StepLR(\n",
    "    optimizer = optimizer,\n",
    "    step_size = 10,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edefe873-c24b-450e-822d-fe4d8a1ddf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                | 0/10000 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "\n",
      "                                                                                                               \u001b[A"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch_tokenized, labels \u001b[38;5;241m=\u001b[39m batch_tokenized[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m loss \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m----> 5\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m      7\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt_bert_env/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt_bert_env/lib/python3.12/site-packages/torch/autograd/__init__.py:260\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    251\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    252\u001b[0m     (inputs,)\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    257\u001b[0m )\n\u001b[1;32m    259\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 260\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/anaconda3/envs/gpt_bert_env/lib/python3.12/site-packages/torch/autograd/__init__.py:133\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    134\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         )\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m    137\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "for texts in tqdm(movRevTrainDL, leave=False):\n",
    "    batch_tokenized = tokenizer(texts ,max_length=1024,padding=\"max_length\", truncation = True, return_tensors='pt').to(device)\n",
    "    output = model(**batch_tokenized, labels = batch_tokenized['input_ids'])\n",
    "    loss = output.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    progress_bar.update\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075cd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in movRevTrainDL:\n",
    "        input_tokens = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        outputs = model(input_tokens, attention_mask=attention_mask)\n",
    "        print(outputs)\n",
    "        loss.backward()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce47b1c9-edf6-4ff4-9526-89d9852d290c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT BERT Env",
   "language": "python",
   "name": "gpt_bert_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
